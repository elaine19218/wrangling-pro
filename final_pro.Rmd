---
title: " Assessment on Impact of COVID-19 pandemic"
author: "Yunjiao Bai"
date: "5/2/2020"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(lubridate)
library(stringr)
library(dplyr)
library(leaflet)
library(gridExtra)
library(rvest)
```

## Abstraction
  
This report focuses on showing the changes in New York City before and after the virus outbreak from two aspects: daily travel and tourism. It also compares New York's climate data with trends in confirmed cases, and uses news headlines to show the popularity of current topics. A simple analysis shows that until the outbreak in March 2020, there is no big difference between the two periods. There is also no evidence that there is an obvious relationship between virus transmission and temperature.  
  
  
## Introduction
  
Since the beginning of this year, the outbreak of coronavirus has affected the entire world. In the United States, due to the high mobility and density of the population, New York has become the most affected state. In this report, I mainly focus on showing the changes in New York City before and after the virus outbreak from two aspects: transportation and tourism. In addition, I compared New York â€™s climate data with trends in confirmed cases to find out whether the virus transmission is related to the weather. Finally, I used the news headlines to show the current topic popularity.  
  
  
## Daily Travel
  
In New York City, most people do not drive because of congestion and parking problems. Citi Bike is the bike sharing system in New York City and the largest bike sharing system in the United States. Therefore, I chose to use the historical data provided by them to estimate people's daily travle. Since the data set is too large to even upload to the cloud, I only choose to compare the same time period in two years: March 2019 and March 2020.  
  
There are 15 variables in the data file and the variables used in this report are:    
starttime/stoptime: the time when the bike is unlocked/locked (primary key in this data)  
start/stop.station.id: trip started/ended station  
start.station.latitude/lontitude: stations precise position  
start.station.latitude/lontitude: stations precise position  
    
    
```{r, data, echo=FALSE}
dat19=read.csv('201903-citibike-tripdata.csv')
dat20=read.csv('202003-citibike-tripdata.csv')
head(dat19, 5)
```
  
  
First of all, since the data are collected according to the using time, so the number of observations in datasets is the number of users. We can see that the number of users decreased by about 300000 in 2020 than in 2019.  
  
      
```{r, data summary, echo=FALSE}
print(paste("Total number of users in March 2019 and 2020 are:", nrow(dat19), nrow(dat20)))
```
  
  
To better visualize the difference, I separated the starttime column into ymd and detailed time and gave out the corresponding weekday. Also, even there is tripduration column in the data, to make sure the duration time is consistent with the time, I also calculated the trip duration as column dur.  
  
These are two bar graphs, representing the total number of users on working days in 2019 and 2020, respectively. We can see that the maximum number of users in 2019 is greater than 200,000, but in 2020 it is less than 200,000, which is only about 175,000. In 2019, there are a large number of users on Friday and Saturday, but the situation is reversed by 2020. It seems that except for working days, at least the number of people traveling by bicycle is decreasing. Since the end of January, the US government has imposed restrictions on entry. This phenomenon may also be affected by the decrease in the number of tourists.  
  
```{r, weekday plot, echo=FALSE}
########201903
op=options(digits.secs = 3)
bike19=dat19 %>%
  mutate(start=ymd_hms(starttime, tz='America/New_York'),
         stop=ymd_hms(stoptime, tz='America/New_York'),
         dur=as.numeric(stop-start)*60, 
         weekday=weekdays(start)) %>%
  separate(start, into = c("start_ymd", "start_time"), sep = " ")

options(op)
#sum(duplicated(bike19)) #0: there's no duplicates

#number of users in weekdays--barplots
bike19$weekday=bike19$weekday %>%
  factor(levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))

barplot1=bike19 %>%
  group_by(weekday) %>%
  summarise(n=n()) %>%
  ggplot(mapping=aes(x=weekday, y=n)) +
  geom_bar(stat="identity", fill="lightcyan2") +
  ggtitle("201903")


#######202003
op=options(digits.secs = 3)
bike20=dat20 %>%
  mutate(start=ymd_hms(starttime, tz='America/New_York'),
         stop=ymd_hms(stoptime, tz='America/New_York'),
         dur=as.numeric(stop-start)*60, 
         weekday=weekdays(start)) %>%
  separate(start, into = c("start_ymd", "start_time"), sep = " ")

options(op)
#sum(duplicated(bike20)) #0 so there

#number of users in weekdays--barplots
bike20$weekday=bike20$weekday %>%
  factor(levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))

barplot2=bike20 %>%
  group_by(weekday) %>%
  summarise(n=n()) %>%
  ggplot(mapping=aes(x=weekday, y=n)) +
  geom_bar(stat="identity", fill="lightcyan2") +
  ggtitle("202003")

head(bike19, 5)
barplot1
barplot2
```
  
  
As for the trip duration, on average it increased about 300 seconds.  
The graph below shows the everage tripduration for every weekday in March 2019 and March 2020. It can be seen that, the trip duration decreases as a whole in 2020.  
  
  
```{r, tripduration summary}
print(c('201903', summary(bike19$dur)))
print(c('202003', summary(bike20$dur)))
print(paste("Average trip duration in March 2019 and March 2020 are:", mean(bike19$dur), mean(bike20$dur)))

duration19=bike19 %>% 
  select(weekday, dur) %>% 
  group_by(weekday) %>% 
  summarise(average=mean(dur)) %>% 
  mutate(time=rep('201903', times=7))
duration20=bike20 %>% 
  select(weekday, dur) %>% 
  group_by(weekday) %>% 
  summarise(average=mean(dur)) %>% 
  mutate(time=rep('202003', times=7))
duration=rbind(duration19, duration20)
duration %>% 
  ggplot(aes(x=weekday, y=average, color=time, group=time)) +
  geom_point()+
  geom_line()

```
  
  
Furthermore, after splitting the related street name from the original column, I extracted all the streets name from data. By combining frequency for every street in both start and stop column, I got frequency for all the streets in 2019 and 2020. The top 10 streets are reserved to get a better illustration.  Then, under the help of function gather(), I the plot like this. Top 10 busy streets are: Broadway, 6 Ave, 1 Ave, 8 Ave, 2 Ave, 5 Ave, 3 Ave, 7 Ave, 10 Ave and Park Ave.  
  
From the plot, we can easily see that stations on the same busy streets in 2020 are used less frequently than in 2019. Combining the graph above, people are going out less frequently but the tripduration rises up.  
  
  
```{r, busy streets summary}
##19
bike19$start.station.name=as.character(bike19$start.station.name)
bike19$end.station.name=as.character(bike19$end.station.name)
streets19=c(bike19$start.station.name, bike19$end.station.name) %>%
  str_split(" & ") %>%
  unlist() %>%
  tibble(name=.) %>%
  group_by(name) %>%
  summarise(n19=n()) %>%
  arrange(desc(n19)) 


##20
bike20$start.station.name=as.character(bike20$start.station.name)
bike20$end.station.name=as.character(bike20$end.station.name)
streets20=c(bike20$start.station.name, bike20$end.station.name) %>%
  str_split(" & ") %>%
  unlist() %>%
  tibble(name=.) %>%
  group_by(name) %>%
  summarise(n20=n()) %>%
  arrange(desc(n20)) 

x=streets19 %>% head(10)
y=streets20 %>% head(10)
top10 = x %>%
  left_join(y, by='name') %>% 
  gather(year, n, n19:n20)

x
y
top10 %>% 
  ggplot(aes(x=name, y=n, color=year, group=year)) +
  geom_point()+
  geom_line()
```
  
  
    
  
In addition, there are two interactive maps created by leaflets(Google map may have better performance but Google map did not provide free API any more).    
  
The first map is for 2019, and the second one is for 2020. The color on the map denotes the station's frequency. There are four categories: Low(white), Normal(yellow), Middle(orange) and High(red). They are divided by using the quantiles of frequency in 2019.   
  
From 2019 to 2020, there's actually little difference existing except for some stations deleted by the company. However, still in the north of the map, the red color seems to be covered a little by the orange and yellow ones, which means stations in those parts are not used as often as a year before.  
  
  
Map: 201903  
  
```{r, frequency leaflet for 2019, echo=FALSE}
#leaflet of stations2019
start.station19= dat19 %>%
  select(start.station.id, start.station.latitude, 
         start.station.longitude)

stop.station19=dat19 %>%
  select(end.station.id, end.station.latitude, 
         end.station.longitude) 

names(start.station19)=c("id", "lat", "lon")
names(stop.station19)=c("id", "lat", "lon")
full19=rbind(start.station19, stop.station19) 
full19$id=as.integer(full19$id)


id19=full19 %>%
  group_by(id) %>%
  summarise(ntotal=n())

stations19=full19 %>%
  left_join(id19, by='id') %>%
  filter(!duplicated(.)==TRUE)

#quantile(stations19$ntotal)

stations19$type = cut(stations19$ntotal, 
                      breaks = c(1, 1038.75, 2295, 4944.75, 21808), 
                      right=FALSE,
                      labels = c("Low", "Normal", "Middle", "High"))  
pal <- colorFactor(c("oldlace", "lightgoldenrod", "orange", "red"), 
                   domain = c("Low", "Normal", "Middle", "High"))

leaflet(stations19) %>% addTiles() %>%
  addCircleMarkers(~lon, ~lat, radius = 6,
                   color = ~pal(type),
                   stroke = FALSE, fillOpacity = 0.5
  )
```
  
  
Map: 202003  
  
```{r, frequency leaflet for 2020, echo=FALSE}


#leaflet of stations2020
#only want stations with more than median frequency:815
start.station20= dat20 %>%
  select(start.station.id, start.station.latitude, 
         start.station.longitude)

stop.station20=dat20 %>%
  select(end.station.id, end.station.latitude, 
         end.station.longitude) 

names(start.station20)=c("id", "lat", "lon")
names(stop.station20)=c("id", "lat", "lon")
full20=rbind(start.station20, stop.station20) 
full20$id=as.integer(full20$id)


id20=full20 %>%
  group_by(id) %>%
  summarise(ntotal=n())

stations20=full20 %>%
  left_join(id20, by='id') %>%
  filter(!duplicated(.)==TRUE)



stations20$type = cut(stations20$ntotal, 
                      breaks = c(1, 1038.75, 2295, 4944.75, 21808), 
                      right=FALSE,
                      labels = c("Low", "Normal", "Middle", "High"))  
pal <- colorFactor(c("oldlace", "lightgoldenrod", "orange", "red"), 
                   domain = c("Low", "Normal", "Middle", "High"))

leaflet(stations20) %>% addTiles() %>%
  addCircleMarkers(~lon, ~lat, radius = 6,
                   color = ~pal(type),
                   stroke = FALSE, fillOpacity = 0.5
  )
  


```
  
  
  
## Tourism  
  
The dataset in this part comes from Airbnb, which is a famous online marketplace for arranging or offering lodging, primarily homestays, or tourism experiences. Since Airbnb does not have the data about occupancy, and on their official page, they used review rate to estimate bookings, here I also used the number of reviews to represent the number of travelers in each year because we will mostly focus on the relative trend rather than the precise number.  
  
Initially, I planned to collect at least three years of data to analyze, but still, the data file is so big, so I just compared the same period of two years. The original data file(bnb_dat) contains all the comments for every listing house from about January 2009 to March 2020. For convenience, I separated the date column into the year, month, and day. To compare the changes brought about by the panic, I filtered the data into two datasets: bnb_dat201903 and bnb_dat202003 and both of them contain data from January to March in each year, which is a quarter of the year.  
  
There are 9 variables in the origin data:  
listing_id: the identifier for each listed house  
id: comment id  
date: comment date  
reviewer_id: identifier for reviewers  
reviewer_name: name of reviewer  
  
```{r, data preparation, echo=FALSE}
#bnb_dat=read.csv('2020reviews.csv')
#bnb_dat2019=bnb_dat %>%
#  mutate(time=ymd(date)) %>%
#  filter(year(time)==2019) %>%
#  separate(time, into = c("year", "month", "day"), sep = "-")
#write_csv(bnb_dat2019, "bnb2019")  
#bnb_dat2020=bnb_dat %>%
#  mutate(time=ymd(date)) %>%
#  filter(year(time)==2020) %>%
#  separate(time, into = c("year", "month", "day"), sep = "-")
#write_csv(bnb_dat2020, "bnb2020")  

bnb_dat2019=read.csv('bnb2019')
bnb_dat2020=read.csv('bnb2020')

bnb_dat2019$month=as.integer(bnb_dat2019$month)
bnb_dat2020$month=as.integer(bnb_dat2020$month)

#filter data in first season
bnb_dat201903=bnb_dat2019 %>%
  filter(month<=3) 
  
bnb_dat202003=bnb_dat2020 %>%
  filter(month<=3)

head(bnb_dat201903, 3)

```
  
  
Here are two bar plots for the number of comments in 2019 and 2020. We can see that the trend in 2019 is first going down and then rising up in March. The largest number is about 30000 in March. However, in 2020, the trend is obviously going down although the largest number of comments is more than 30000.  
  
  
```{r, visualize monthly reviews, echo=FALSE}
#barplot for the number of comments in each month
p1=bnb_dat201903 %>%
  group_by(month) %>%
  summarise(n=n()) %>%
  ggplot(aes(x=month, y=n)) +
  geom_bar(stat="identity", fill="lightcyan2") +
  ggtitle("201903")

p2=bnb_dat202003 %>%
  group_by(month) %>%
  summarise(n=n()) %>%
  ggplot(aes(x=month, y=n)) +
  geom_bar(stat="identity", fill="lightcyan2") +
  ggtitle("202003")

grid.arrange(p1, p2, ncol=2)
```
  
  
Below are some simple descriptive indicators. Compared to the same period in 2019, the number of reviews increased by about 23%. Only in comments in March appears the related words 'covid', 'COVID', and so on. And the rate of those comments is just 1.1%. But since apart from English, many comments using other languages might also mention the virus, so the real mention rate may be a bit higher.  
  
  
```{r, covid detection in reviews, echo=FALSE}
# the number of reviews decreased by about 70%
print(paste("The number of reviews in 2019 and 2020 are: ", 
        nrow(bnb_dat201903), nrow(bnb_dat202003)))
decrease_rate=(nrow(bnb_dat202003)-nrow(bnb_dat201903))/nrow(bnb_dat201903)
print(paste("Growth rate: ", decrease_rate*100))

## covid in reviews in March
bnb_dat2020_3 = bnb_dat202003 %>%
  filter(month==3) %>%
  mutate(covid=str_detect(comments, "covid|COVID|coronavirus"))

s=bnb_dat2020_3[bnb_dat2020_3$covid==TRUE, ]
p=sum(bnb_dat2020_3$covid, na.rm = TRUE)/nrow(bnb_dat2020_3)
head(s, 5)
print(paste("Rate of comments having covid related topic:", p))

```
  
  
  
## Weather  
  
Since the webpage I originally planned to collect monthly historical weather information has now changed its format, I obtained the climate data from another webpage. According to the table, temperatures in New York City generally increase from January to July. If high temperatures can help control the spread of the virus, at least the number of confirmed cases should be reduced. But so far, the number of diagnoses is still increasing. Of course, here is just a simple analysis. A more technical analysis may give a more accurate and correct explanation.  
  
  
```{r, weather information scraping, echo=FALSE}
weather="https://www.usclimatedata.com/climate/new-york/new-york/united-states/usny0996" %>%
  read_html %>%
  html_table()
t1=weather[1] %>%
  unlist() 

t2=weather[2] %>%
  unlist() %>%
  .[-1:-5]
w=c(t1, t2)
name_w=names(w)[-1:-5]
weather_dat=matrix(0, 5, 13) 
month_name=name_w %>%
  str_extract("[A-z]{1,3}") %>%
  unique()
colnames(weather_dat)=c("indicater", month_name)
weather_dat[1, ] = w[str_detect(name_w, ".1$")]
weather_dat[2, ] = w[str_detect(name_w, ".2$")]
weather_dat[3, ] = w[str_detect(name_w, ".3$")]
weather_dat[4, ] = w[str_detect(name_w, ".4$")]
weather_dat[5, ] = w[str_detect(name_w, ".5$")]

knitr::kable(weather_dat)
```


  
## News Headlines  
  
Here are some headlines I scraped from Google headlines. Although the node is quite easy to find, the cleaning process is really hard. Since there are many buttons on the page and different buttons have different names in the raw webpage I scraped, it's really time-consuming to figure out every type of buttons representatives and make their regular expressions in codes.   
  
The output below shows us, at this time how many news headlines mentioned COVID. We can see that even the coronavirus outbreak has been happening for almost a half year, it is still a hot topic, which is consistent with the ever-growing number of confirmed cases.  
  
  
  
```{r, news headlines scraping}
criteria0=paste("bookmark_bordersharemore_vert","play_arrow",
                "keyboard_arrow_rightplay_arrow","View Full Coveragekeyboard_arrow_up","View Full Coverage", sep="|")
headlines <- "https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRFZxYUdjU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl=US&ceid=US%3Aen" %>%
  read_html() %>%
  html_nodes(".bWfURe") %>%
  html_text() %>%
  str_split(criteria0) %>%
  unlist() %>%
  str_replace_all("...ampvideo_youtube", " ") 
headlines=headlines[headlines!=""][-1]

criteria=paste("[[:digit:]]+[[:space:]]{1}[a-z]+[[:space:]]{1}ago$",
                "Yesterday",
                "[[:digit:]]+[[:space:]]{1}[a-z]+[[:space:]]{1}agoOpinion",
                "[[:digit:]]+[[:space:]]{1}[a-z]+[[:space:]]{1}agoLocal[[:space:]]{1}coverage", sep="|")

post_info=headlines %>% 
  str_extract_all(criteria) %>%
  unlist()
#success=headlines %>% 
#  str_detect(criteria)
#headlines[success==FALSE]

tb_headlines=tibble(title=headlines, time=post_info) 
covid=tb_headlines$title %>% 
  str_detect('Coronavirus|(?i)covid') 
print(paste("The number of headlines related with covid is: ", sum(covid)))
output=tb_headlines[covid, ] %>% 
  head(5)
knitr::kable(output)
```
  
  
  
## Conclusions  
  
In terms of daily travel and tourism in New York, at least up to March 2019, there's no obvious change in people's daily travel since the stay-at-home order was executed on March 22 which is almost the end of March.  
  
As for the statement that higher temperatures will control the spread of the virus, from the climate data collected in this report, there's no evidence to prove it.  
What's more, with the increasing number of confirmed cases, the topic of coronavirus will keep being popular for a long time.  
  
  
  
## Appendices  
  
Data origin:  
Daily travel: https://www.citibikenyc.com/system-data  
Tourism: http://insideairbnb.com/get-the-data.html  
Weather: https://www.usclimatedata.com/climate/new-york/new-york/united-states/usny0996  
News headlines: https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRFZxYUdjU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl=US&ceid=US%3Aen  














